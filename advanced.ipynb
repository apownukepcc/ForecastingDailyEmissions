{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oVZqaSRotXD",
        "outputId": "7c2e5d60-4ad9-4e81-80ee-99fe0975f743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error on Test Data: 0.000176\n",
            "Selected Dates for Prediction: ['2022-05-01', '2022-01-01', '2022-01-02']\n",
            "Average Emissions/Load for Unit LAKE-1: 0.000034 tons/MW\n",
            "Average Emissions/Load for Unit LAKE-2: 0.000048 tons/MW\n",
            "Average Emissions/Load for Unit LAKE-3: 0.000043 tons/MW\n",
            "\n",
            "The most efficient unit is LAKE-1 with an average Emissions/Load of 0.000034 tons/MW.\n",
            "\n",
            "Date: 2022-05-01 | Unit: LAKE-1\n",
            "Predicted Emissions/Load: 0.000026 tons/MW\n",
            "Actual Emissions/Load: 0.000012 tons/MW\n",
            "\n",
            "Date: 2022-01-01 | Unit: LAKE-1\n",
            "Predicted Emissions/Load: 0.000028 tons/MW\n",
            "Actual Emissions/Load: 0.000018 tons/MW\n",
            "\n",
            "Date: 2022-01-02 | Unit: LAKE-1\n",
            "Predicted Emissions/Load: 0.000049 tons/MW\n",
            "Actual Emissions/Load: 0.000046 tons/MW\n",
            "\n",
            "Date: 2022-05-01 | Unit: LAKE-2\n",
            "Predicted Emissions/Load: 0.000048 tons/MW\n",
            "Actual Emissions/Load: 0.000061 tons/MW\n",
            "\n",
            "Date: 2022-01-01 | Unit: LAKE-3\n",
            "Predicted Emissions/Load: 0.000038 tons/MW\n",
            "Actual Emissions/Load: 0.000024 tons/MW\n",
            "\n",
            "Date: 2022-01-02 | Unit: LAKE-3\n",
            "Predicted Emissions/Load: 0.000047 tons/MW\n",
            "Actual Emissions/Load: 0.000047 tons/MW\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn import __version__ as sklearn_version\n",
        "from packaging import version\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"https://raw.githubusercontent.com/apownukepcc/ForecastingDailyEmissions/refs/heads/main/combinedWeatherValues.csv\"  # Replace with the actual path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert 'date' column to datetime\n",
        "data['date'] = pd.to_datetime(data['date'])\n",
        "\n",
        "# Define emissions parameters and load\n",
        "emissions_params = ['SO2TONS', 'NOXTONS', 'COTONS']\n",
        "load_param = 'LOADMWBA'\n",
        "\n",
        "# Separate emissions and load data\n",
        "emissions_data = data[data['Parameter'].isin(emissions_params)]\n",
        "load_data = data[data['Parameter'] == load_param]\n",
        "\n",
        "# Merge emissions and load data on date and source (use a left join to retain emissions data)\n",
        "merged_data = pd.merge(\n",
        "    emissions_data,\n",
        "    load_data,\n",
        "    on=[\"date\", \"Source\"],\n",
        "    suffixes=(\"_emission\", \"_load\"),\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Calculate emissions/load only for rows where both emissions and load are available\n",
        "merged_data[\"Emissions_Load\"] = merged_data[\"Value_emission\"] / merged_data[\"Value_load\"]\n",
        "\n",
        "# Drop rows with NaN in Emissions_Load\n",
        "merged_data = merged_data.dropna(subset=[\"Emissions_Load\"])\n",
        "\n",
        "# Define predictors and target\n",
        "predictors = ['tavg_emission', 'tmin_emission', 'tmax_emission', 'prcp_emission',\n",
        "              'snow_emission', 'wdir_emission', 'wspd_emission', 'pres_emission']\n",
        "categorical_features = ['Source', 'Parameter_emission', 'Units_emission']\n",
        "target = 'Emissions_Load'\n",
        "\n",
        "# Dynamic OneHotEncoder\n",
        "if version.parse(sklearn_version) >= version.parse(\"1.2\"):\n",
        "    one_hot_encoder = OneHotEncoder(drop=\"first\", sparse_output=False)  # For newer versions\n",
        "else:\n",
        "    one_hot_encoder = OneHotEncoder(drop=\"first\")  # For older versions (before 1.2)\n",
        "\n",
        "# Preprocess categorical and numeric features\n",
        "column_transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', one_hot_encoder, categorical_features),\n",
        "        ('num', StandardScaler(), predictors)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Prepare features and target\n",
        "X = merged_data[predictors + categorical_features]\n",
        "y = merged_data[target]\n",
        "\n",
        "# Encode features\n",
        "X_encoded = column_transformer.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Random Forest model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error on Test Data: {mae:.6f}\")\n",
        "\n",
        "# Function to find valid dates for prediction\n",
        "def find_valid_dates(season_filter, num_days):\n",
        "    if season_filter == 'peak':\n",
        "        season_data = merged_data[merged_data['date'].dt.month.isin([5, 6, 7, 8])]  # Peak Season\n",
        "    elif season_filter == 'off-peak':\n",
        "        season_data = merged_data[merged_data['date'].dt.month.isin([1, 2, 3, 4, 9, 10, 11, 12])]  # Off-Peak Season\n",
        "    else:\n",
        "        season_data = merged_data\n",
        "\n",
        "    unique_dates = season_data['date'].unique()\n",
        "    return [str(date.date()) for date in unique_dates[:num_days]]\n",
        "\n",
        "# Forecast emissions for multiple dates and analyze efficiency\n",
        "def forecast_and_analyze_efficiency():\n",
        "    # Select one Peak Season date and two Off-Peak Season dates\n",
        "    peak_date = find_valid_dates('peak', num_days=1)\n",
        "    off_peak_dates = find_valid_dates('off-peak', num_days=2)\n",
        "    selected_dates = peak_date + off_peak_dates\n",
        "\n",
        "    print(f\"Selected Dates for Prediction: {selected_dates}\")\n",
        "\n",
        "    efficiency_results = {}\n",
        "    results = []\n",
        "    for unit in merged_data['Source'].unique():\n",
        "        unit_results = []\n",
        "        for date_to_predict in selected_dates:\n",
        "            selected_day = merged_data[(merged_data['date'] == pd.Timestamp(date_to_predict)) &\n",
        "                                        (merged_data['Source'] == unit)]\n",
        "            if selected_day.empty:\n",
        "                continue\n",
        "            selected_day = selected_day.iloc[0]\n",
        "\n",
        "            # Extract features for the selected day\n",
        "            selected_day_features = pd.DataFrame([{\n",
        "                **{col: selected_day[col] for col in predictors},\n",
        "                **{col: selected_day[col] for col in categorical_features}\n",
        "            }])\n",
        "\n",
        "            # Transform features for prediction\n",
        "            selected_day_encoded = column_transformer.transform(selected_day_features)\n",
        "            predicted_emissions_load = model.predict(selected_day_encoded)[0]\n",
        "\n",
        "            unit_results.append(predicted_emissions_load)\n",
        "\n",
        "            # Collect actual values for comparison\n",
        "            results.append({\n",
        "                \"Unit\": unit,\n",
        "                \"Date\": date_to_predict,\n",
        "                \"Predicted Emissions/Load\": predicted_emissions_load,\n",
        "                \"Actual Emissions/Load\": selected_day[\"Emissions_Load\"]\n",
        "            })\n",
        "\n",
        "        # Calculate efficiency for each unit (average emissions/load)\n",
        "        if unit_results:\n",
        "            avg_efficiency = sum(unit_results) / len(unit_results)\n",
        "            efficiency_results[unit] = avg_efficiency\n",
        "            print(f\"Average Emissions/Load for Unit {unit}: {avg_efficiency:.6f} tons/MW\")\n",
        "\n",
        "    # Identify the most efficient unit\n",
        "    most_efficient_unit = min(efficiency_results, key=efficiency_results.get)\n",
        "    print(f\"\\nThe most efficient unit is {most_efficient_unit} with an average Emissions/Load of \"\n",
        "          f\"{efficiency_results[most_efficient_unit]:.6f} tons/MW.\")\n",
        "\n",
        "    # Display results\n",
        "    for result in results:\n",
        "        print(f\"\\nDate: {result['Date']} | Unit: {result['Unit']}\")\n",
        "        print(f\"Predicted Emissions/Load: {result['Predicted Emissions/Load']:.6f} tons/MW\")\n",
        "        print(f\"Actual Emissions/Load: {result['Actual Emissions/Load']:.6f} tons/MW\")\n",
        "\n",
        "# Run the forecasting and efficiency analysis\n",
        "forecast_and_analyze_efficiency()\n"
      ]
    }
  ]
}